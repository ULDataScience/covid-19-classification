{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "covid_19_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDPG6OF-l0ln"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this jupyter notebook, a CNN will be trained to classify Chest X-ray (CXR) images, whether they are diagnosed with COVID-19 or have diagnosis (NO FINDING). For this purpose, two seperate datasets are used. One that contains COVID-19 CXR images and another containing CXR images with no diagnosis (see below). Furthermore, a segmentation is applied with a U-Net to mask out parts of images irrelevant to a COVID-19 diagnosis (see below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs2OrysKMRLy"
      },
      "source": [
        "# Preparation of tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMTd0xGPqYNM"
      },
      "source": [
        "## Import of required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2nlKlJqPobm",
        "outputId": "ba6b1196-7062-4a29-f464-4c96545c7770"
      },
      "source": [
        "# Install required packages\n",
        "!pip install tensorflow_addons\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import dill\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from skimage import color, exposure, io, morphology, transform\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications import NASNetLarge\n",
        "from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint,\n",
        "                                        TensorBoard)\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.layers import (Activation, Dense, Dropout,\n",
        "                                     GlobalMaxPooling2D, Input, LeakyReLU)\n",
        "from tensorflow.keras.metrics import (AUC, BinaryAccuracy,\n",
        "                                      CategoricalCrossentropy, FalseNegatives,\n",
        "                                      FalsePositives,\n",
        "                                      MeanAbsolutePercentageError, Precision,\n",
        "                                      Recall, SensitivityAtSpecificity,\n",
        "                                      SpecificityAtSensitivity, TrueNegatives,\n",
        "                                      TruePositives)\n",
        "from tensorflow.keras.models import Model, load_model, save_model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow_addons.metrics import F1Score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muCpS2PlLSEA"
      },
      "source": [
        "[ChestX-ray8.tar](https://drive.google.com/file/d/1jqaMwk-VVvr7snrqcsGL_gj7TXp6-oWB/view?usp=sharing) must be added to your Google Drive as a shortcut. You possibly have to modify the path in line four on the cell below depending on where you have placed the shortcut of the `tar` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXKZzfBjjE8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca8ffc3-5541-44c6-ed7b-2e1ce70bbe29"
      },
      "source": [
        "drive_path = ('drive')\n",
        "drive.mount(os.path.join(os.getcwd(), drive_path))\n",
        "\n",
        "tar = tarfile.open('/content/drive/My Drive/Colab Notebooks/datasets/ChestX-ray8.tar', 'r')\n",
        "tar.extractall(os.path.join(os.getcwd(), 'data'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEbY2Y0ZLLsK"
      },
      "source": [
        "[](https://drive.google.com/file/d/1jqaMwk-VVvr7snrqcsGL_gj7TXp6-oWB/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKFMCsCkMk4k"
      },
      "source": [
        "## Cloning necessary repositories \n",
        "- [covid-19-chestxray-dataset](https://github.com/ieee8023/covid-chestxray-dataset) into `data/covid-chestxray-dataset` is used for Chest X-rays with COVID-19 Diagnosis\n",
        "- [lung-segmentation-2d](https://github.com/imlab-uiip/lung-segmentation-2d) into `tools/lung-segmentation-2d` is used to apply lung segmentation to the Chest X-ray images and create lung masks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woeJbITfZ_NM",
        "outputId": "668c0cb0-0551-4f33-efb0-ea51cfd295d6"
      },
      "source": [
        "!git clone https://github.com/ieee8023/covid-chestxray-dataset.git data/covid-chestxray-dataset\n",
        "!git clone https://github.com/imlab-uiip/lung-segmentation-2d.git tools/lung-segmentation-2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3ckqlchM-9_"
      },
      "source": [
        "The configuration is defined in the following cell as a `dict`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI_7uxDdUKIX"
      },
      "source": [
        "drive_path = ('drive')\n",
        "config = {\n",
        "  'PATHS': {\n",
        "    'RAW_DATA': os.path.join(os.getcwd(), 'data'),\n",
        "    'COVID_CHEST_XRAY_DATA': os.path.join(os.getcwd(), 'data', 'covid-chestxray-dataset'),\n",
        "    'CHEST_XRAY_8_DATA': os.path.join(os.getcwd(), 'data', 'ChestX-ray8'),\n",
        "    'PROCESSED_DATA': os.path.join(os.getcwd(), 'data', 'processed'),\n",
        "    'TRAIN_SET': os.path.join(os.getcwd(), 'data', 'processed', 'train_set.csv'),\n",
        "    'VAL_SET': os.path.join(os.getcwd(), 'data', 'processed', 'val_set.csv'),\n",
        "    'TEST_SET': os.path.join(os.getcwd(), 'data', 'processed', 'test_set.csv'),\n",
        "    'IMAGES': os.path.join(os.getcwd(), drive_path, 'My Drive', 'Colab Notebooks', 'output', 'documents', 'generated_images'),\n",
        "    'LOGS': os.path.join(os.getcwd(), 'results', 'logs'),\n",
        "    'MODELS_FOLDER': os.path.join(os.getcwd(), drive_path, 'My Drive', 'Colab Notebooks', 'models', 'model_covid'),\n",
        "    'OUTPUT_CLASS_INDICES': os.path.join(os.getcwd(), 'data', 'interpretability', 'output_class_indices.pkl'),\n",
        "    'UNET_MODEL_PATH': os.path.join(os.getcwd(), 'tools', 'lung-segmentation-2d', 'trained_model.hdf5')\n",
        "  },\n",
        "  'DATA': {\n",
        "    'IMG_DIM': tuple([331])*2,\n",
        "    'VIEW': 'PA',\n",
        "    'VAL_SPLIT_PERCENT': 0.08,\n",
        "    'TEST_SPLIT_PERCENT': 0.1,\n",
        "    'NUM_CHEST_XRAY_8_IMAGES': 1000,\n",
        "    'RESIZE_WITH_PADDING': True,\n",
        "    'CLASSES': [\n",
        "      'NO FINDING',\n",
        "      'COVID-19'\n",
        "    ],\n",
        "    'OTHER_CONTAINS_ONLY_HEALTHY': True,\n",
        "    # One of 'class_weight' or 'reduce'\n",
        "    'CLASS_BALANCE_STRATEGY': ['class_weight', 'reduce'][1]\n",
        "  },\n",
        "  'SEGMENTATION': {\n",
        "    'IMG_DIM': tuple([256])*2,\n",
        "    'MORPHOLOGY_KERNEL_SIZE': tuple([5])*2,\n",
        "    'DILATION_KERNEL_SIZE': tuple([2])*2,\n",
        "    'DILATION_ITERATIONS': 3,\n",
        "    'MASK_BINARIZATION_TRESHOLD': 0.25\n",
        "  },\n",
        "  'TRAIN': {\n",
        "    'BATCH_SIZE': 32,\n",
        "    'FT_BATCH_SIZE': 16,\n",
        "    'EPOCHS': 150,\n",
        "    'USE_MASKED_IMAGES': True,\n",
        "    'THRESHOLDS': 0.5,\n",
        "    'ENABLE_EARLY_STOPPING': True,\n",
        "    'PATIENCE_FOR_EARLY_STOPPING': 10,\n",
        "    'NUM_GPUS': 1\n",
        "  },\n",
        "  'NN': {\n",
        "    'NODES_DENSE0': 256,\n",
        "    'LR': 0.00001,\n",
        "    'FT_LR': 0.000001,\n",
        "    'OPTIMIZER': 'adam',\n",
        "    'DROPOUT': 0.3,\n",
        "    'L2_LAMBDA': 0.0001\n",
        "  },\n",
        "  'PREDICTION': {\n",
        "    'THRESHOLD': 0.5\n",
        "  }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGRR1q4TZi3Q"
      },
      "source": [
        "# Create folders if not yet created\n",
        "for path in config['PATHS']:\n",
        "  if not bool(re.match('^.*\\.[a-zA-Z0-9]+$', config['PATHS'][path])):\n",
        "    Path(config['PATHS'][path]).mkdir(parents=True, exist_ok=True)\n",
        "  else:\n",
        "    splitted_path = config['PATHS'][path][:config['PATHS'][path].rfind('/')]\n",
        "    Path(splitted_path).mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9obHBhnoosQ"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29UqR7PyZiTm"
      },
      "source": [
        "# process datasets in pandas for futher preprocessing\n",
        "covid_chest_xray_path = config['PATHS']['COVID_CHEST_XRAY_DATA']\n",
        "chest_xray_8_path = config['PATHS']['CHEST_XRAY_8_DATA']\n",
        "\n",
        "covid_chest_xray_df = pd.read_csv(os.path.join(covid_chest_xray_path, 'metadata.csv'))\n",
        "covid_chest_xray_df['filename'] = [os.path.join(covid_chest_xray_path, 'images', row) for row in covid_chest_xray_df['filename'].astype(str)]\n",
        "\n",
        "covid_views_cxrs_df = covid_chest_xray_df['view'].str.match(config['DATA']['VIEW'])\n",
        "covid_pos_df = covid_chest_xray_df['finding'].str.contains('COVID-19')\n",
        "covid_df = covid_chest_xray_df[covid_pos_df & covid_views_cxrs_df] \n",
        "\n",
        "chest_xray_8_df = pd.read_csv(os.path.join(chest_xray_8_path, 'subset.csv'))\n",
        "num_chest_xray_8_imgs = config['DATA']['NUM_CHEST_XRAY_8_IMAGES']\n",
        "chest_xray_8_normal_df = chest_xray_8_df[chest_xray_8_df['Finding Labels'].str.match('No Finding')]\n",
        "chest_xray_8_pneum_df = chest_xray_8_df[chest_xray_8_df['Finding Labels'].str.match('(?!No Finding)')]\n",
        "\n",
        "chest_xray_8_normal_sample_df = chest_xray_8_normal_df.sample(frac = num_chest_xray_8_imgs / chest_xray_8_normal_df.shape[0], random_state=num_chest_xray_8_imgs)\n",
        "\n",
        "chest_xray_8_pneum_sample_df = chest_xray_8_pneum_df.sample(frac = num_chest_xray_8_imgs / chest_xray_8_pneum_df.shape[0], random_state=num_chest_xray_8_imgs)\n",
        "\n",
        "if config['DATA']['OTHER_CONTAINS_ONLY_HEALTHY']:\n",
        "  chest_xray_8_df = chest_xray_8_normal_sample_df\n",
        "else:\n",
        "  chest_xray_8_df = pd.concat([chest_xray_8_normal_sample_df, chest_xray_8_pneum_sample_df], axis=0)\n",
        "\n",
        "chest_xray_8_df['filename'] = [os.path.join(chest_xray_8_path, row) for row in chest_xray_8_df['Image Index'].astype(str)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bScrMfmBSgu"
      },
      "source": [
        "# define labels and concatinate datasets\n",
        "covid_df = covid_df.assign(label='COVID-19')\n",
        "chest_xray_8_df = chest_xray_8_df.assign(label='NO FINDING')\n",
        "\n",
        "chest_xray_8_selected_df = None\n",
        "if config['DATA']['CLASS_BALANCE_STRATEGY'] == 'reduce':\n",
        "  chest_xray_8_selected_df = chest_xray_8_df.head(covid_df.shape[0])\n",
        "else:\n",
        "  chest_xray_8_selected_df = chest_xray_8_df\n",
        "\n",
        "file_df = pd.concat(\n",
        "        [covid_df[['filename', 'label']],\n",
        "        chest_xray_8_selected_df[['filename', 'label']]], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-224fm6p3YI"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyzCjDlMSC6B"
      },
      "source": [
        "A padding is added to the images in order to bring them to an `n*n` dimension without distortion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it7KNaRBfwZ5"
      },
      "source": [
        "def resize_with_pad(file_path, img_size):\n",
        "  img = Image.open(file_path)\n",
        "  old_size = img.size  \n",
        "  ratio = float(img_size) / max(old_size)\n",
        "  new_size = tuple([int(x * ratio) for x in old_size])\n",
        "\n",
        "  img = img.resize(new_size, Image.ANTIALIAS)\n",
        "\n",
        "  new_img = Image.new(\"RGB\", (img_size, img_size))\n",
        "  new_img.paste(img, ((img_size - new_size[0]) // 2,\n",
        "                  (img_size - new_size[1]) // 2))\n",
        "  return new_img\n",
        "\n",
        "for file_path in file_df['filename']:\n",
        "  if config['DATA']['RESIZE_WITH_PADDING']:\n",
        "    padded_img = resize_with_pad(file_path, config['DATA']['IMG_DIM'][0])\n",
        "    padded_img.save(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8clYDJWp5uz"
      },
      "source": [
        "## Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TO6O8B7SS6n"
      },
      "source": [
        "Mask the lungs with the help of the U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8zxadbd8kqn",
        "outputId": "8aa87800-d940-47ce-ebbd-2d5bed3095ee"
      },
      "source": [
        "UNet = load_model(config['PATHS']['UNET_MODEL_PATH'])\n",
        "\n",
        "def mask_image(image, mask):\n",
        "    \"\"\"Returns masked image\"\"\"\n",
        "    return np.ma.masked_where(mask == 0, image)\n",
        "     \n",
        "\n",
        "def remove_small_regions_and_dilate(image):\n",
        "    \"\"\"Morphologically removes small (less than kernel size) connected regions of 0s or 1s and dilates mask\"\"\"\n",
        "\n",
        "    morphology_kernel = np.ones(config['SEGMENTATION']['MORPHOLOGY_KERNEL_SIZE'], np.uint8)\n",
        "    dilation_kernel = np.ones(config['SEGMENTATION']['DILATION_KERNEL_SIZE'], np.uint8)\n",
        "    image = np.squeeze(image).astype(np.float32)\n",
        "    image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, morphology_kernel)\n",
        "    image = cv2.morphologyEx(image, cv2.MORPH_OPEN, morphology_kernel)\n",
        "    image = cv2.dilate(image, dilation_kernel, iterations = config['SEGMENTATION']['DILATION_ITERATIONS'])\n",
        "\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9EqJJBLwyC8"
      },
      "source": [
        "masked_filenames = []\n",
        "\n",
        "for row in file_df.iterrows():\n",
        "  file_name = row[1][0]\n",
        "  dot_index = file_name.rfind('.')\n",
        "  masked_image_filename = '{}{}{}'.format(file_name[:dot_index], '_masked', file_name[dot_index:])\n",
        "  mask_filename = '{}{}{}'.format(file_name[:dot_index], '_mask', file_name[dot_index:])\n",
        "  original_image = cv2.imread(file_name, 0).astype(np.float32)/255.0\n",
        "  original_image_size = original_image.shape[::-1]\n",
        "  \n",
        "  downsized_image = cv2.resize(original_image, dsize=config['SEGMENTATION']['IMG_DIM'], interpolation=cv2.INTER_CUBIC)\n",
        "  downsized_image = np.expand_dims(downsized_image, axis=0)\n",
        "  downsized_image = tf.image.per_image_standardization(downsized_image)\n",
        "\n",
        "  mask_prediction = UNet.predict(downsized_image)\n",
        "\n",
        "  mask = mask_prediction > config['SEGMENTATION']['MASK_BINARIZATION_TRESHOLD']\n",
        "  mask = remove_small_regions_and_dilate(mask)\n",
        "  upsized_mask = cv2.resize(np.squeeze(mask).astype(np.float32), dsize=original_image_size, interpolation=cv2.INTER_CUBIC)\n",
        "  masked_image = mask_image(original_image, upsized_mask)\n",
        "  cv2.imwrite(mask_filename, upsized_mask*255)\n",
        "  cv2.imwrite(masked_image_filename, masked_image*255)\n",
        "  masked_filenames.append(masked_image_filename)\n",
        "  \n",
        "file_df.insert(1, 'masked_filename', masked_filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHIEY9KLp-Td"
      },
      "source": [
        "## Dataset split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpxT1kWwSgsp"
      },
      "source": [
        "Create Train, Test and Validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc46c8sfxNX2"
      },
      "source": [
        "validation_split_size = config['DATA']['VAL_SPLIT_PERCENT']\n",
        "test_split_size = config['DATA']['TEST_SPLIT_PERCENT']\n",
        "file_df_train, file_df_test = train_test_split(file_df, test_size=test_split_size, stratify=file_df['label'], random_state=42)\n",
        "relative_validation_split_size = validation_split_size / (1 - test_split_size)\n",
        "file_df_train, file_df_val = train_test_split(file_df_train, test_size=relative_validation_split_size,\n",
        "                                                    stratify=file_df_train['label'], random_state=42)\n",
        "\n",
        "if not os.path.exists(config['PATHS']['PROCESSED_DATA']):\n",
        "    os.makedirs(config['PATHS']['PROCESSED_DATA'])\n",
        "file_df_train.to_csv(config['PATHS']['TRAIN_SET'])\n",
        "file_df_val.to_csv(config['PATHS']['VAL_SET'])\n",
        "file_df_test.to_csv(config['PATHS']['TEST_SET'])\n",
        "\n",
        "data = {}\n",
        "data['TRAIN'] = pd.read_csv(config['PATHS']['TRAIN_SET'])\n",
        "data['VAL'] = pd.read_csv(config['PATHS']['VAL_SET'])\n",
        "data['TEST'] = pd.read_csv(config['PATHS']['TEST_SET'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qOmJdSldpiz"
      },
      "source": [
        "Remove images selected for the demonstrator from training dataset and replace them with images from the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZuTQsApsWIj",
        "outputId": "d6e43159-f368-4c58-9969-a5a1ccf690dc"
      },
      "source": [
        "demonstrator_image_list = [\"00008720_000.png\", '23E99E2E-447C-46E5-8EB2-D35D12473C39.png', '14d81f378173b86cc53f21d2d67040_jumbo.jpeg', '6C94A287-C059-46A0-8600-AFB95F4727B7.jpeg', '16660_4_1.jpg', '16660_5_1.jpg'] \n",
        "is_in_train = [train_image for train_image in list(data['TRAIN']['filename']) for demonstrator_image in demonstrator_image_list if demonstrator_image in str(train_image)]\n",
        "is_in_train\n",
        "is_in_val = [val_image for val_image in list(data['VAL']['filename']) for demonstrator_image in demonstrator_image_list if demonstrator_image in str(val_image)]\n",
        "print(is_in_train)\n",
        "print(is_in_val)\n",
        "\n",
        "for file_path in is_in_train:\n",
        "  data['TEST'] = data['TEST'].append(data['TRAIN'].loc[data['TRAIN']['filename'] == file_path])\n",
        "  data['TRAIN'] = data['TRAIN'].loc[data['TRAIN']['filename'] != file_path]\n",
        "  data['TRAIN'] = data['TRAIN'].append(data['TEST'].iloc[0])\n",
        "  data['TEST'] = data['TEST'].tail(-1)\n",
        "\n",
        "for file_path in is_in_val:\n",
        "  data['TEST'] = data['TEST'].append(data['VAL'].loc[data['VAL']['filename'] == file_path])\n",
        "  data['VAL'] = data['VAL'].loc[data['VAL']['filename'] != file_path]\n",
        "  data['VAL'] = data['VAL'].append(data['TEST'].iloc[0])\n",
        "  data['TEST'] = data['TEST'].tail(-1)\n",
        "\n",
        "demonstrator_image_list = [\"00008720_000.png\", '23E99E2E-447C-46E5-8EB2-D35D12473C39.png', '14d81f378173b86cc53f21d2d67040_jumbo.jpeg', '6C94A287-C059-46A0-8600-AFB95F4727B7.jpeg', '16660_4_1.jpg', '16660_5_1.jpg'] \n",
        "is_in_train = [train_image for train_image in list(data['TRAIN']['filename']) for demonstrator_image in demonstrator_image_list if demonstrator_image in str(train_image)]\n",
        "is_in_train\n",
        "is_in_test = [val_image for val_image in list(data['TEST']['filename']) for demonstrator_image in demonstrator_image_list if demonstrator_image in str(val_image)]\n",
        "print(len(is_in_test) == len(demonstrator_image_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtbuUukYwdF1"
      },
      "source": [
        "Save datasets to csv files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rMg8BhSwceT"
      },
      "source": [
        "if not os.path.exists(config['PATHS']['PROCESSED_DATA']):\n",
        "    os.makedirs(config['PATHS']['PROCESSED_DATA'])\n",
        "file_df_train.to_csv(config['PATHS']['TRAIN_SET'])\n",
        "file_df_val.to_csv(config['PATHS']['VAL_SET'])\n",
        "file_df_test.to_csv(config['PATHS']['TEST_SET'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpFwwH93So84"
      },
      "source": [
        "Here we define the callbacks for the training. They prevent overfitting and generally spare time when the further training of model does not improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-beDuTPotQL"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgLE5Kv7_Oc6"
      },
      "source": [
        "cur_date = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "log_dir = os.path.join(config['PATHS']['LOGS'], 'training', cur_date)\n",
        "model_filename_single = 'weights.hdf5'\n",
        "if not os.path.exists(os.path.join(config['PATHS']['LOGS'], 'training')):\n",
        "    os.makedirs(os.path.join(config['PATHS']['LOGS'], 'training'))\n",
        "callbacks = []\n",
        "if config['TRAIN']['ENABLE_EARLY_STOPPING']:\n",
        "  early_stopping = EarlyStopping(\n",
        "    monitor='val_auc_pr',\n",
        "    verbose=1, \n",
        "    patience=config['TRAIN']['PATIENCE_FOR_EARLY_STOPPING'], \n",
        "    mode='max', \n",
        "    restore_best_weights=True)\n",
        "  callbacks.append(early_stopping)\n",
        "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "callbacks.append(tensorboard)\n",
        "model_checkpoint = ModelCheckpoint(model_filename_single, \n",
        "                                   monitor='val_auc_pr', \n",
        "                                   verbose=1, \n",
        "                                   save_best_only=True,\n",
        "                                   save_weights_only=True, \n",
        "                                   mode='max', \n",
        "                                   save_freq='epoch'\n",
        ")\n",
        "callbacks.append(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uNxhxYSTacT"
      },
      "source": [
        "Create `ImageDataGenerators` for the training, validation, and test datasets. Hereby the training dataset is augmented with random rotations of 10 degrees in both directions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        },
        "id": "hykWVJWW_ToM",
        "outputId": "996e55c5-860c-4f92-d258-2123b876be32"
      },
      "source": [
        "train_img_gen = ImageDataGenerator(rotation_range=10, samplewise_std_normalization=True, samplewise_center=True)\n",
        "val_img_gen = ImageDataGenerator(samplewise_std_normalization=True, samplewise_center=True)\n",
        "test_img_gen = ImageDataGenerator(samplewise_std_normalization=True, samplewise_center=True)\n",
        "\n",
        "img_shape = config['DATA']['IMG_DIM']\n",
        "\n",
        "file_column = 'masked_filename' if config['TRAIN']['USE_MASKED_IMAGES'] else 'filename'\n",
        "\n",
        "class_mode = 'binary'\n",
        "train_generator = train_img_gen.flow_from_dataframe(\n",
        "    dataframe=data['TRAIN'],\n",
        "    x_col=file_column,\n",
        "    y_col='label',\n",
        "    target_size=img_shape,\n",
        "    batch_size=config['TRAIN']['BATCH_SIZE'],\n",
        "    class_mode=class_mode,\n",
        "    validate_filenames=True)\n",
        "val_generator = val_img_gen.flow_from_dataframe(\n",
        "    dataframe=data['VAL'],\n",
        "    x_col=file_column,\n",
        "    y_col='label',\n",
        "    target_size=img_shape,\n",
        "    batch_size=config['TRAIN']['BATCH_SIZE'],\n",
        "    class_mode=class_mode,\n",
        "    validate_filenames=True)\n",
        "test_generator = test_img_gen.flow_from_dataframe(\n",
        "    dataframe=data['TEST'],\n",
        "    x_col=file_column,\n",
        "    y_col='label',\n",
        "    target_size=img_shape,\n",
        "    batch_size=config['TRAIN']['BATCH_SIZE'],\n",
        "    class_mode=class_mode,\n",
        "    validate_filenames=True,\n",
        "    shuffle=False)\n",
        "\n",
        "dill.dump(test_generator.class_indices, open(config['PATHS']['OUTPUT_CLASS_INDICES'], 'wb+'))\n",
        "\n",
        "histogram = np.bincount(np.array(train_generator.labels).astype(int))\n",
        "\n",
        "class_weight = None\n",
        "if config['DATA']['CLASS_BALANCE_STRATEGY'] == 'class_weight':\n",
        "  class_multiplier_list = [min(histogram) / max(histogram)]\n",
        "  class_multiplier_list.insert(int(histogram[0] > histogram[1]), 1.0)\n",
        "\n",
        "  class_multiplier = [\n",
        "          class_multiplier_list[config['DATA']['CLASSES'].index(c)]\n",
        "              for c in test_generator.class_indices\n",
        "  ]\n",
        "\n",
        "  weights = [(1.0 / len(histogram)) * sum(histogram) / histogram[i] for i in range(len(histogram))]\n",
        "\n",
        "  class_weight = {i: class_multiplier[i] for i in range(len(histogram))}  \n",
        "\n",
        "x,y = train_generator.next()\n",
        "train_generator.reset()\n",
        "\n",
        "fig, ax = plt.subplots(2,4)\n",
        "fig.set_size_inches(15,15)\n",
        "num = 0\n",
        "for i in range(2):\n",
        "  for j in range(4):\n",
        "    ax[i,j].imshow(x[num])\n",
        "    ax[i,j].axis('off')\n",
        "    num += 1\n",
        "        \n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PAcjXPhU6N5"
      },
      "source": [
        "## Defining the model and the training metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keFLWsOB_inH",
        "outputId": "70bd11bf-8c07-45ec-844c-8ea29ac1b441"
      },
      "source": [
        "covid_class_idx = test_generator.class_indices['COVID-19']   \n",
        "thresholds = 1.0 / len(config['DATA']['CLASSES'])\n",
        "metrics = [BinaryAccuracy(name='binary_accuracy'),\n",
        "    Precision(name='precision', thresholds=thresholds, class_id=covid_class_idx),\n",
        "    Recall(name='recall', thresholds=thresholds, class_id=covid_class_idx),\n",
        "    AUC(name='auc_pr', curve='PR'),\n",
        "    AUC(name='auc_roc', curve='ROC'),\n",
        "    F1Score(name='f1score', threshold=thresholds, num_classes=1), \n",
        "    TrueNegatives(name='tn'), \n",
        "    TruePositives(name='tp'), \n",
        "    FalseNegatives(name='fn'), \n",
        "    FalsePositives(name='fp'),\n",
        "    SpecificityAtSensitivity(sensitivity=thresholds, name='speAtSen'),\n",
        "    SensitivityAtSpecificity(specificity=thresholds, name='senAtSpe')]\n",
        "\n",
        "input_shape = config['DATA']['IMG_DIM']+tuple([3])\n",
        "num_gpus = config['TRAIN']['NUM_GPUS']\n",
        "\n",
        "model_config = config['NN']\n",
        "\n",
        "nodes_dense0 = model_config['NODES_DENSE0']\n",
        "lr = model_config['LR']\n",
        "dropout = model_config['DROPOUT']\n",
        "l2_lambda = model_config['L2_LAMBDA']\n",
        "\n",
        "if model_config['OPTIMIZER'] == 'sgd':\n",
        "    optimizer = SGD(learning_rate=lr)\n",
        "else:\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "\n",
        "histogram = np.bincount([config['DATA']['CLASSES'].index(label) for label in data['TRAIN']['label'].astype(str)])\n",
        "output_bias = np.log([histogram[i] / (np.sum(histogram) - histogram[i]) for i in range(histogram.shape[0])])\n",
        "\n",
        "# Set output bias\n",
        "if output_bias is not None:\n",
        "    output_bias = Constant(output_bias)\n",
        "print(\"MODEL CONFIG: \", model_config)\n",
        "\n",
        "X_input = Input(input_shape, name='input_img')\n",
        "base_model = NASNetLarge(include_top=False, weights='imagenet', input_shape=input_shape, input_tensor=X_input)\n",
        "base_model.trainable = False\n",
        "X = base_model(X_input, training=False)\n",
        "\n",
        "# Add custom top\n",
        "X = GlobalMaxPooling2D()(X)\n",
        "X = Dropout(dropout)(X)\n",
        "X = Dense(nodes_dense0, kernel_initializer='he_uniform', activity_regularizer=l2(l2_lambda))(X)\n",
        "X = LeakyReLU()(X)\n",
        "X = Dense(1)(X)\n",
        "Y = Activation('sigmoid', dtype='float32', name='output')(X)\n",
        "\n",
        "model = Model(inputs=X_input, outputs=Y)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZVdy9ntpzWo"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmgFbZPUBYEh",
        "outputId": "f2666ecb-98c0-4642-9351-ad25d35b40bd"
      },
      "source": [
        "# Train model and save the training history\n",
        "history = model.fit(train_generator, \n",
        "                    epochs=config['TRAIN']['EPOCHS'],        \n",
        "                    validation_data=val_generator,\n",
        "                    callbacks=callbacks,\n",
        "                    class_weight=class_weight,\n",
        "                    verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyUI0G1mpjk9"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtF4zrbMbsGd"
      },
      "source": [
        "# save history to json:\n",
        "hist_df = pd.DataFrame(history.history) \n",
        "hist_json_file = '/content/history.json'\n",
        "with open(hist_json_file, mode='w') as f:\n",
        "    hist_df.to_json(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wztq2rLnrY7",
        "outputId": "ab92f8d1-48e5-40a1-fe26-b9ac928af76e"
      },
      "source": [
        "#  evaluate model and save evaluation result to json:  \n",
        "evaluation = model.evaluate(test_generator)\n",
        "evaluation_data_df = pd.DataFrame(evaluation) \n",
        "\n",
        "evaluation_json_file = '/content/evaluation.json'\n",
        "with open(evaluation_json_file, mode='w') as f:\n",
        "    evaluation_data_df.to_json(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C-eRe2WWPku"
      },
      "source": [
        "Save model to given Google Drive path with current data in the `config` dict and print file path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "09OipdO36Md1",
        "outputId": "65d7e07b-8f72-480b-8fe0-3175e371a105"
      },
      "source": [
        "model_path = os.path.join(config['PATHS']['MODELS_FOLDER'], '{}{}{}'.format('model', cur_date, '.h5'))\n",
        "save_model(model, model_path)\n",
        "model_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpFzjdPvaKBY"
      },
      "source": [
        "Save predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ7WBfs8gypk"
      },
      "source": [
        "test_predictions = model.predict(test_generator)\n",
        "test_labels = test_generator.labels\n",
        "train_predictions = model.predict(train_generator)\n",
        "train_labels = train_generator.labels\n",
        "train = pd.DataFrame(zip([i[0] for i in train_predictions], train_labels), columns =['Predictions', 'Labels'])\n",
        "test = pd.DataFrame(zip([i[0] for i in test_predictions], test_labels), columns =['Predictions', 'Labels'])\n",
        "train.to_csv('train_predictions.csv')\n",
        "test.to_csv('test_predictions.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CvtKHecpcvz"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajbY--PLW6j3"
      },
      "source": [
        "Initalize fine tuning callbacks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX1jJAepXxV0"
      },
      "source": [
        "cur_date = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "log_dir = os.path.join(config['PATHS']['LOGS'], 'training_ft', cur_date)\n",
        "model_filename_ft = 'fine_tuning_weights.hdf5'\n",
        "\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "callbacks_ft = []\n",
        "early_stopping_ft = EarlyStopping(\n",
        "  monitor='val_auc_pr',\n",
        "  verbose=1, \n",
        "  patience=3, \n",
        "  mode='max', \n",
        "  restore_best_weights=True)\n",
        "\n",
        "callbacks_ft.append(early_stopping_ft)\n",
        "\n",
        "tensorboard_ft = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "callbacks_ft.append(tensorboard_ft)\n",
        "model_checkpoint_ft = ModelCheckpoint(model_filename_ft, \n",
        "                                   monitor='val_auc_pr', \n",
        "                                   verbose=1, \n",
        "                                   save_best_only=True,\n",
        "                                   save_weights_only=True, \n",
        "                                   mode='max', \n",
        "                                   save_freq='epoch', \n",
        ")\n",
        "callbacks_ft.append(model_checkpoint_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls-1G4c_j_yX",
        "outputId": "0ff61249-3b0c-4735-de1e-e2790e3a4fa9"
      },
      "source": [
        "file_column = 'masked_filename' if config['TRAIN']['USE_MASKED_IMAGES'] else 'filename'\n",
        "\n",
        "class_mode = 'binary'\n",
        "train_generator = train_img_gen.flow_from_dataframe(\n",
        "    dataframe=data['TRAIN'],\n",
        "    x_col=file_column,\n",
        "    y_col='label',\n",
        "    target_size=img_shape,\n",
        "    batch_size=config['TRAIN']['FT_BATCH_SIZE'],\n",
        "    class_mode=class_mode,\n",
        "    validate_filenames=True)\n",
        "val_generator = val_img_gen.flow_from_dataframe(\n",
        "    dataframe=data['VAL'],\n",
        "    x_col=file_column,\n",
        "    y_col='label',\n",
        "    target_size=img_shape,\n",
        "    batch_size=config['TRAIN']['FT_BATCH_SIZE'],\n",
        "    class_mode=class_mode,\n",
        "    validate_filenames=True)\n",
        "test_generator = test_img_gen.flow_from_dataframe(\n",
        "    dataframe=data['TEST'],\n",
        "    x_col=file_column,\n",
        "    y_col='label',\n",
        "    target_size=img_shape,\n",
        "    batch_size=config['TRAIN']['FT_BATCH_SIZE'],\n",
        "    class_mode=class_mode,\n",
        "    validate_filenames=True,\n",
        "    shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfD73F2OsbOI"
      },
      "source": [
        "model.get_layer(index=1).trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8oixd4rW_g8"
      },
      "source": [
        "Configure the model, so that not only the custom top is trained, but also other parameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkEu0gT2X6Z4",
        "outputId": "652d73b0-1ad5-478e-fae7-ae18d079fbcc"
      },
      "source": [
        "#base_model.trainable = True\n",
        "\n",
        "lr = model_config['FT_LR']\n",
        "\n",
        "if model_config['OPTIMIZER'] == 'sgd':\n",
        "    optimizer = SGD(learning_rate=lr)\n",
        "else:\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=metrics)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A4aMmRqpquM"
      },
      "source": [
        "## Fine-tuning evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3cQmftIXNBh"
      },
      "source": [
        "Fine tune the model for maximum 30 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pozcd_31Yg0N",
        "outputId": "790a1bf2-0c80-4031-9eec-2e656d941327"
      },
      "source": [
        "fine_tune_epochs = 30\n",
        "last_epoch = history.epoch[-1] - config['TRAIN']['PATIENCE_FOR_EARLY_STOPPING'] + 1\n",
        "total_epochs = last_epoch + fine_tune_epochs\n",
        "\n",
        "history_fine = model.fit(train_generator, \n",
        "                          epochs=total_epochs,                          \n",
        "                          initial_epoch=last_epoch,\n",
        "                          validation_data=val_generator, \n",
        "                          callbacks=callbacks_ft,\n",
        "                          verbose=True,\n",
        "                          class_weight=class_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7QHLwbNXSWx"
      },
      "source": [
        "Evaluate fine tuned model and save results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbUyFYlQbwt8"
      },
      "source": [
        "# save fine-tuning history to json:  \n",
        "\n",
        "hist_ft_df = pd.DataFrame(history_fine.history) \n",
        "\n",
        "hist_ft_json_file = '/content/history_ft.json'\n",
        "with open(hist_ft_json_file, mode='w') as f:\n",
        "    hist_ft_df.to_json(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZT6_nG9b2Go",
        "outputId": "8e5b061e-8967-4276-b513-d5ae43d3d08c"
      },
      "source": [
        "# save evaluation to json:  \n",
        "evaluation = model.evaluate(test_generator)\n",
        "\n",
        "evaluation_data_df = pd.DataFrame(evaluation) \n",
        "\n",
        "evaluation_json_file = '/content/evaluation_ft.json'\n",
        "with open(evaluation_json_file, mode='w') as f:\n",
        "    evaluation_data_df.to_json(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aRipEYNXisN"
      },
      "source": [
        "Save fine tuned model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NcO51NjEOJok",
        "outputId": "56f4104f-2281-4b7e-af25-271cb2e76e3b"
      },
      "source": [
        "model_path = os.path.join(config['PATHS']['MODELS_FOLDER'], '{}{}{}'.format('model_ft', cur_date, '.h5'))\n",
        "save_model(model, model_path)\n",
        "model_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LY2vMvvpLwP"
      },
      "source": [
        "test_predictions = model.predict(test_generator)\n",
        "test_labels = test_generator.labels\n",
        "train_predictions = model.predict(train_generator)\n",
        "train_labels = train_generator.labels\n",
        "train = pd.DataFrame(zip([i[0] for i in train_predictions], train_labels), columns =['Predictions', 'Labels'])\n",
        "test = pd.DataFrame(zip([i[0] for i in test_predictions], test_labels), columns =['Predictions', 'Labels'])\n",
        "train.to_csv('train_predictions_ft.csv')\n",
        "test.to_csv('test_predictions_ft.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcAJh5rnlZ3I"
      },
      "source": [
        "Copy result to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5C5SWFEySVm"
      },
      "source": [
        "!mv evaluation.json evaluation_ft.json history.json history_ft.json train_predictions.csv test_predictions.csv train_predictions_ft.csv test_predictions_ft.csv results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzgb335mzZHF"
      },
      "source": [
        "!cp -r results \"/content/drive/My Drive/Colab Notebooks/models/model_covid/\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}